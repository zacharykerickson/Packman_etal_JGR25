{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cbcaf1f-664e-4d7e-8ce2-2b43964b8675",
   "metadata": {},
   "source": [
    "Import packages and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c778fd8e-30cb-4854-a030-8c401dc16ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import xarray as xa\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy.stats import beta\n",
    "from statsmodels.tsa import stattools\n",
    "from netCDF4 import Dataset\n",
    "from glob import glob\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3de3b6e7-c94f-4520-8e6a-9377517fc495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_decimal_year(time):\n",
    "    dt = pd.to_datetime(time) # converts np.datetime64 into datetime object\n",
    "    dec_year = []\n",
    "    for i in range(len(dt)):\n",
    "        start = datetime.date(dt[i].year,1,1).toordinal()\n",
    "        year_length = datetime.date(dt[i].year+1, 1, 1).toordinal() - start\n",
    "        dec_year.append(dt[i].year + (float(dt[i].toordinal() - start) / year_length)) # calculates how much time has passed in the year, divides by year length, then adds to start year\n",
    "    return np.array(dec_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ee6a0d2-c4e4-4469-9d1f-e1bf8b8d4c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_decimal_year(time):\n",
    "    dt = pd.to_datetime(time) # converts np.datetime64 into datetime object\n",
    "    dec_year = []\n",
    "    for i in range(len(dt)):\n",
    "        start = datetime.date(dt[i].year,1,1).toordinal()\n",
    "        year_length = datetime.date(dt[i].year+1, 1, 1).toordinal() - start\n",
    "        dec_year.append(dt[i].year + (float(dt[i].toordinal() - start) / year_length)) # calculates how much time has passed in the year, divides by year length, then adds to start year\n",
    "    return np.array(dec_year)\n",
    "def lag_corr(array1,array2,lag,T=None,return_autocorrelation_lengthscale = False,nlags=1000):\n",
    "    if T is None:\n",
    "        T = (2*np.max(np.cumsum(stattools.acf(array1,nlags=nlags)))+2*np.max(np.cumsum(stattools.acf(array2,nlags=nlags))))\n",
    "    if lag>0:\n",
    "        r = pearsonr(array2[lag:-1],array1[:-1-lag])[0]\n",
    "    elif lag<0:\n",
    "        l = int(-1.*lag)\n",
    "        r = pearsonr(array1[l:-1],array2[:-1-l])[0]\n",
    "    else:\n",
    "        r = pearsonr(array1,array2)[0]\n",
    "    n = (len(array1)-lag)*2/T - 14 # 1 for mean, 1 for trend, 6*2 for 6 seasonal harmonics\n",
    "    dist = beta(n/2-1,n/2-1,loc=-1,scale=2)\n",
    "    p = 2*dist.cdf(-abs(r))\n",
    "    if return_autocorrelation_lengthscale:\n",
    "        return r,p,T\n",
    "    else:\n",
    "        return r,p\n",
    "def detrend_from_coefs(time,values,coefs):\n",
    "    yr = to_decimal_year(time)\n",
    "    \n",
    "    mean = coefs[0]\n",
    "    trend = coefs[1]*(yr-np.mean(yr))\n",
    "    res_coefs = coefs[2:]\n",
    "    \n",
    "    detrended = values.copy()\n",
    "    detrended -= mean\n",
    "    detrended -= trend\n",
    "    for j in range(0,len(res_coefs),2):\n",
    "        omega = (j+2)*np.pi\n",
    "        detrended -= (res_coefs[j]*np.sin(omega*yr))+(res_coefs[j+1]*np.cos(omega*yr))\n",
    "    \n",
    "    return detrended  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb0efc7-3df9-4a19-8e21-c364a38c6f62",
   "metadata": {},
   "source": [
    "Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "090cfe41-7dad-4d6c-9fdf-f4ba687de98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = np.sort(glob('/Users/erickson/Documents/Data/RFROM/*.nc'))\n",
    "data = xa.open_mfdataset(files)\n",
    "time = data['time'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08e5218c-5fdb-4deb-8de6-eb91f7648981",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = Dataset('trend.nc','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1281e7e-9738-453d-b5e9-8b7e47683075",
   "metadata": {},
   "outputs": [],
   "source": [
    "dep = 150\n",
    "depind = np.where(np.isin(data.variables['mean_pressure'].values,dep))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0d47edf-5be9-47c9-ab28-944e76eb793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = np.arange(-52*3,52*3+1)\n",
    "lats = data['latitude'].values\n",
    "lons = data['longitude'].values\n",
    "lagged_corr = np.empty(shape=(len(lats),len(lons),len(lags)))*np.nan\n",
    "lagged_p = np.empty(shape=(len(lats),len(lons),len(lags)))*np.nan\n",
    "lagged_n = np.empty(shape=(len(lats),len(lons),len(lags)))*np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "077fabbf-5bab-4858-a46c-d81e657c4f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_out = Dataset('lagged_correlation_analysis_150m_final.nc','w')\n",
    "nc_out.createDimension('lag',len(lags))\n",
    "nc_out.createDimension('lat',len(lats))\n",
    "nc_out.createDimension('lon',len(lons))\n",
    "var = nc_out.createVariable('lag','i8',('lag'))\n",
    "var.units = 'weeks'\n",
    "var[:] = lags\n",
    "nc_out.createVariable('lat','f4',('lat'))[:] = lats\n",
    "nc_out.createVariable('lon','f4',('lon'))[:] = lons\n",
    "nc_out.createVariable('lagged_corr','f4',('lag','lat','lon'))\n",
    "nc_out.createVariable('lagged_p','f4',('lag','lat','lon'))\n",
    "nc_out.createVariable('lagged_n','f4',('lag','lat','lon'))\n",
    "nc_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dea2cc97-c361-4d89-982f-af8fd2fcd69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_corr(array1,array2,lag,nlags=200):\n",
    "    T1 = 2*np.max(np.cumsum(stattools.acf(array1,nlags=nlags)))\n",
    "    T2 = 2*np.max(np.cumsum(stattools.acf(array2,nlags=nlags)))\n",
    "    \n",
    "    if lag>0:\n",
    "        r = pearsonr(array2[lag:-1],array1[:-1-lag])[0]\n",
    "    elif lag<0:\n",
    "        l = int(-1.*lag)\n",
    "        r = pearsonr(array1[l:-1],array2[:-1-l])[0]\n",
    "    else:\n",
    "        r = pearsonr(array1,array2)[0]\n",
    "    l = len(array1)-np.abs(lag)\n",
    "    n = (l/T1 + l/T2) - 8\n",
    "    dist = beta(n/2-1,n/2-1,loc=-1,scale=2)\n",
    "    p = 2*dist.cdf(-abs(r))\n",
    "    return r,p,n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8086efd-5a3c-4611-9615-d36a141ce7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(lats<-50)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed9304a5-5fa6-44a6-a0d4-041f63d59a09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 40/40 [7:36:15<00:00, 684.39s/it]\n"
     ]
    }
   ],
   "source": [
    "for j in tqdm(range(40)): #tqdm(range(len(lats))):\n",
    "    for k in range(len(lons)):\n",
    "        if np.isnan(coefs.variables['coefs'][0,0,j,k]):\n",
    "            continue\n",
    "        if np.any(np.isnan(coefs.variables['coefs'][:,0,j,k])) or np.any(np.isnan(coefs.variables['coefs'][:,depind,j,k])):\n",
    "            continue\n",
    "        surf_data = detrend_from_coefs(time,data.variables['ocean_temperature'][:,0,j,k].values,coefs.variables['coefs'][:,0,j,k])\n",
    "        dep_data = detrend_from_coefs(time,data.variables['ocean_temperature'][:,depind,j,k].values,coefs.variables['coefs'][:,depind,j,k])\n",
    "        if np.any(np.isnan(dep_data)) or np.any(np.isinf(dep_data)) or np.any(np.isnan(surf_data)) or np.any(np.isinf(surf_data)):\n",
    "            continue\n",
    "        for l in range(len(lags)):\n",
    "            lag = lags[l]\n",
    "            lagged_corr,lagged_p,lagged_n = lag_corr(surf_data,dep_data,lag);\n",
    "            nc_out = Dataset('lagged_correlation_analysis_150m_final.nc','a')\n",
    "            nc_out.variables['lagged_corr'][l,j,k] = lagged_corr\n",
    "            nc_out.variables['lagged_p'][l,j,k] = lagged_p\n",
    "            nc_out.variables['lagged_n'][l,j,k] = lagged_n\n",
    "            nc_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
